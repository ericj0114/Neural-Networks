{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#adapted from https://github.com/eriklindernoren/Keras-GAN\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        #initialize size of target images, image channels, and input shape\n",
    "        self.img_rows = 256\n",
    "        self.img_cols = 256\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        \n",
    "        #define what kind of optimizer to use\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=3, sample_interval=50):\n",
    "\n",
    "        # Load the dataset from saved image directory\n",
    "        images = []\n",
    "        for image_path in os.listdir('xray/data/val/pneumonia'):\n",
    "            img = load_img('xray/data/val/pneumonia' + '/' + image_path, grayscale=True, \n",
    "                                target_size = (self.img_rows, self.img_cols))\n",
    "            img = img_to_array(img)\n",
    "            images.append(img)\n",
    "        X_train = np.array(images)\n",
    "        \n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            \n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5 \n",
    "        noise = np.random.normal(0, 1, (r*c,self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        \n",
    "        for j in range(c):\n",
    "            plt.imsave('gen_pneumonia/%d.png' % epoch, gen_imgs[j, :,:,0], cmap='gray')\n",
    "        #fig, axs = plt.subplots(r, c)\n",
    "        #cnt = 0\n",
    "        #for i in range(r):\n",
    "            #for j in range(c):\n",
    "                #axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                #axs[i,j].axis('off')\n",
    "                #cnt += 1\n",
    "        #fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 512)               33554944  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 33,686,529\n",
      "Trainable params: 33,686,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 65536)             67174400  \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 256, 256, 1)       0         \n",
      "=================================================================\n",
      "Total params: 67,864,320\n",
      "Trainable params: 67,860,736\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.910852, acc.: 46.88%] [G loss: 0.720071]\n",
      "1 [D loss: 0.332791, acc.: 78.12%] [G loss: 0.654915]\n",
      "2 [D loss: 0.317538, acc.: 75.00%] [G loss: 0.933819]\n",
      "3 [D loss: 0.464822, acc.: 59.38%] [G loss: 1.628279]\n",
      "4 [D loss: 0.461647, acc.: 75.00%] [G loss: 1.763264]\n",
      "5 [D loss: 0.658502, acc.: 70.31%] [G loss: 1.971180]\n",
      "6 [D loss: 0.431190, acc.: 81.25%] [G loss: 1.957197]\n",
      "7 [D loss: 0.372327, acc.: 79.69%] [G loss: 3.337990]\n",
      "8 [D loss: 0.363699, acc.: 87.50%] [G loss: 3.067126]\n",
      "9 [D loss: 1.881390, acc.: 25.00%] [G loss: 4.467052]\n",
      "10 [D loss: 1.176309, acc.: 71.88%] [G loss: 5.254648]\n",
      "11 [D loss: 2.267430, acc.: 73.44%] [G loss: 5.897730]\n",
      "12 [D loss: 1.586424, acc.: 76.56%] [G loss: 5.360776]\n",
      "13 [D loss: 2.986023, acc.: 70.31%] [G loss: 7.270726]\n",
      "14 [D loss: 2.567404, acc.: 75.00%] [G loss: 7.092054]\n",
      "15 [D loss: 2.330686, acc.: 73.44%] [G loss: 5.690690]\n",
      "16 [D loss: 0.223225, acc.: 92.19%] [G loss: 8.908852]\n",
      "17 [D loss: 0.247350, acc.: 93.75%] [G loss: 10.023381]\n",
      "18 [D loss: 2.018118, acc.: 81.25%] [G loss: 7.082534]\n",
      "19 [D loss: 2.093065, acc.: 78.12%] [G loss: 6.501848]\n",
      "20 [D loss: 0.603441, acc.: 89.06%] [G loss: 10.731970]\n",
      "21 [D loss: 1.862008, acc.: 81.25%] [G loss: 6.460401]\n",
      "22 [D loss: 2.260524, acc.: 78.12%] [G loss: 8.332273]\n",
      "23 [D loss: 2.900296, acc.: 70.31%] [G loss: 7.788329]\n",
      "24 [D loss: 1.133748, acc.: 84.38%] [G loss: 11.704377]\n",
      "25 [D loss: 0.603586, acc.: 89.06%] [G loss: 12.027381]\n",
      "26 [D loss: 0.443873, acc.: 92.19%] [G loss: 11.287100]\n",
      "27 [D loss: 0.283524, acc.: 96.88%] [G loss: 11.705878]\n",
      "28 [D loss: 2.775408, acc.: 71.88%] [G loss: 10.159329]\n",
      "29 [D loss: 1.092041, acc.: 87.50%] [G loss: 10.615469]\n",
      "30 [D loss: 7.276982, acc.: 34.38%] [G loss: 8.412363]\n",
      "31 [D loss: 1.836981, acc.: 82.81%] [G loss: 8.633928]\n",
      "32 [D loss: 1.796814, acc.: 82.81%] [G loss: 11.772123]\n",
      "33 [D loss: 0.891008, acc.: 89.06%] [G loss: 11.253118]\n",
      "34 [D loss: 0.380468, acc.: 89.06%] [G loss: 13.196815]\n",
      "35 [D loss: 1.168021, acc.: 85.94%] [G loss: 13.271087]\n",
      "36 [D loss: 2.773863, acc.: 57.81%] [G loss: 8.093732]\n",
      "37 [D loss: 2.670809, acc.: 79.69%] [G loss: 7.836466]\n",
      "38 [D loss: 3.496944, acc.: 76.56%] [G loss: 8.794735]\n",
      "39 [D loss: 3.184669, acc.: 76.56%] [G loss: 8.057463]\n",
      "40 [D loss: 2.852501, acc.: 81.25%] [G loss: 8.214747]\n",
      "41 [D loss: 3.438562, acc.: 78.12%] [G loss: 7.734702]\n",
      "42 [D loss: 3.347929, acc.: 75.00%] [G loss: 8.059967]\n",
      "43 [D loss: 3.487397, acc.: 78.12%] [G loss: 6.827806]\n",
      "44 [D loss: 1.976030, acc.: 87.50%] [G loss: 8.074192]\n",
      "45 [D loss: 3.146204, acc.: 76.56%] [G loss: 7.810832]\n",
      "46 [D loss: 4.107605, acc.: 71.88%] [G loss: 7.454192]\n",
      "47 [D loss: 1.743058, acc.: 87.50%] [G loss: 7.479733]\n",
      "48 [D loss: 3.497085, acc.: 76.56%] [G loss: 8.337156]\n",
      "49 [D loss: 2.916238, acc.: 79.69%] [G loss: 9.320185]\n",
      "50 [D loss: 2.206202, acc.: 78.12%] [G loss: 15.455009]\n",
      "51 [D loss: 7.032622, acc.: 54.69%] [G loss: 12.771544]\n",
      "52 [D loss: 6.171558, acc.: 54.69%] [G loss: 7.479939]\n",
      "53 [D loss: 3.985596, acc.: 75.00%] [G loss: 7.844053]\n",
      "54 [D loss: 3.147973, acc.: 78.12%] [G loss: 6.650126]\n",
      "55 [D loss: 2.527977, acc.: 81.25%] [G loss: 7.051667]\n",
      "56 [D loss: 3.494742, acc.: 78.12%] [G loss: 9.315278]\n",
      "57 [D loss: 3.044506, acc.: 78.12%] [G loss: 8.599594]\n",
      "58 [D loss: 2.676856, acc.: 81.25%] [G loss: 8.700034]\n",
      "59 [D loss: 3.985597, acc.: 75.00%] [G loss: 5.944623]\n",
      "60 [D loss: 3.076671, acc.: 78.12%] [G loss: 8.431712]\n",
      "61 [D loss: 2.997741, acc.: 78.12%] [G loss: 8.854724]\n",
      "62 [D loss: 3.003723, acc.: 79.69%] [G loss: 9.086325]\n",
      "63 [D loss: 4.120903, acc.: 71.88%] [G loss: 9.237341]\n",
      "64 [D loss: 3.888969, acc.: 75.00%] [G loss: 9.183990]\n",
      "65 [D loss: 2.434006, acc.: 81.25%] [G loss: 8.128370]\n",
      "66 [D loss: 1.971283, acc.: 87.50%] [G loss: 8.020103]\n",
      "67 [D loss: 2.451855, acc.: 82.81%] [G loss: 8.273779]\n",
      "68 [D loss: 2.884499, acc.: 78.12%] [G loss: 10.572400]\n",
      "69 [D loss: 2.571146, acc.: 79.69%] [G loss: 13.569434]\n",
      "70 [D loss: 6.025185, acc.: 37.50%] [G loss: 9.570120]\n",
      "71 [D loss: 2.492724, acc.: 84.38%] [G loss: 8.109055]\n",
      "72 [D loss: 3.985623, acc.: 75.00%] [G loss: 7.648736]\n",
      "73 [D loss: 4.364418, acc.: 71.88%] [G loss: 7.789453]\n",
      "74 [D loss: 4.129966, acc.: 71.88%] [G loss: 8.059054]\n",
      "75 [D loss: 2.774651, acc.: 81.25%] [G loss: 8.517586]\n",
      "76 [D loss: 4.199875, acc.: 71.88%] [G loss: 8.844116]\n",
      "77 [D loss: 3.519066, acc.: 75.00%] [G loss: 7.197895]\n",
      "78 [D loss: 2.241898, acc.: 85.94%] [G loss: 7.295192]\n",
      "79 [D loss: 2.244391, acc.: 85.94%] [G loss: 7.802600]\n",
      "80 [D loss: 4.171024, acc.: 73.44%] [G loss: 8.562810]\n",
      "81 [D loss: 2.918796, acc.: 81.25%] [G loss: 9.236856]\n",
      "82 [D loss: 4.335316, acc.: 71.88%] [G loss: 8.313493]\n",
      "83 [D loss: 3.736500, acc.: 76.56%] [G loss: 9.288941]\n",
      "84 [D loss: 3.874772, acc.: 73.44%] [G loss: 8.080442]\n",
      "85 [D loss: 3.041134, acc.: 78.12%] [G loss: 9.008228]\n",
      "86 [D loss: 2.812909, acc.: 79.69%] [G loss: 15.975645]\n",
      "87 [D loss: 6.649705, acc.: 54.69%] [G loss: 15.827675]\n",
      "88 [D loss: 6.056266, acc.: 56.25%] [G loss: 14.990248]\n",
      "89 [D loss: 3.401739, acc.: 73.44%] [G loss: 11.910212]\n",
      "90 [D loss: 2.552128, acc.: 79.69%] [G loss: 13.925220]\n",
      "91 [D loss: 0.694278, acc.: 95.31%] [G loss: 16.118095]\n",
      "92 [D loss: 7.876047, acc.: 43.75%] [G loss: 10.523094]\n",
      "93 [D loss: 2.074150, acc.: 84.38%] [G loss: 11.126702]\n",
      "94 [D loss: 2.290365, acc.: 82.81%] [G loss: 13.846588]\n",
      "95 [D loss: 2.952585, acc.: 68.75%] [G loss: 9.366257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 [D loss: 1.451041, acc.: 90.62%] [G loss: 14.479944]\n",
      "97 [D loss: 2.994485, acc.: 71.88%] [G loss: 8.345007]\n",
      "98 [D loss: 3.238767, acc.: 79.69%] [G loss: 8.044483]\n",
      "99 [D loss: 2.772537, acc.: 81.25%] [G loss: 9.070171]\n",
      "100 [D loss: 2.632624, acc.: 82.81%] [G loss: 8.508446]\n",
      "101 [D loss: 2.525341, acc.: 82.81%] [G loss: 9.378815]\n",
      "102 [D loss: 2.936045, acc.: 79.69%] [G loss: 8.157928]\n",
      "103 [D loss: 1.487056, acc.: 87.50%] [G loss: 8.060826]\n",
      "104 [D loss: 1.368590, acc.: 89.06%] [G loss: 12.391882]\n",
      "105 [D loss: 0.608966, acc.: 95.31%] [G loss: 14.096930]\n",
      "106 [D loss: 1.446750, acc.: 87.50%] [G loss: 14.611010]\n",
      "107 [D loss: 0.382156, acc.: 96.88%] [G loss: 13.932178]\n",
      "108 [D loss: 0.475317, acc.: 92.19%] [G loss: 12.593159]\n",
      "109 [D loss: 1.320806, acc.: 87.50%] [G loss: 13.114674]\n",
      "110 [D loss: 0.387377, acc.: 95.31%] [G loss: 15.624297]\n",
      "111 [D loss: 2.036500, acc.: 71.88%] [G loss: 13.038494]\n",
      "112 [D loss: 0.860162, acc.: 93.75%] [G loss: 11.620022]\n",
      "113 [D loss: 1.262814, acc.: 90.62%] [G loss: 13.261406]\n",
      "114 [D loss: 1.427561, acc.: 90.62%] [G loss: 15.272854]\n",
      "115 [D loss: 0.621291, acc.: 93.75%] [G loss: 13.709071]\n",
      "116 [D loss: 1.416649, acc.: 85.94%] [G loss: 12.271330]\n",
      "117 [D loss: 0.797578, acc.: 93.75%] [G loss: 13.405878]\n",
      "118 [D loss: 0.731859, acc.: 93.75%] [G loss: 14.655448]\n",
      "119 [D loss: 1.058212, acc.: 87.50%] [G loss: 10.240728]\n",
      "120 [D loss: 1.472938, acc.: 90.62%] [G loss: 12.533061]\n",
      "121 [D loss: 0.249651, acc.: 98.44%] [G loss: 12.318434]\n",
      "122 [D loss: 1.288633, acc.: 89.06%] [G loss: 14.375065]\n",
      "123 [D loss: 0.539187, acc.: 93.75%] [G loss: 15.745217]\n",
      "124 [D loss: 4.884759, acc.: 46.88%] [G loss: 10.998802]\n",
      "125 [D loss: 1.743699, acc.: 89.06%] [G loss: 10.410526]\n",
      "126 [D loss: 3.498006, acc.: 78.12%] [G loss: 8.884578]\n",
      "127 [D loss: 2.380242, acc.: 84.38%] [G loss: 8.779915]\n",
      "128 [D loss: 2.778584, acc.: 81.25%] [G loss: 10.779501]\n",
      "129 [D loss: 1.568456, acc.: 89.06%] [G loss: 11.956244]\n",
      "130 [D loss: 1.245810, acc.: 92.19%] [G loss: 13.129669]\n",
      "131 [D loss: 0.747299, acc.: 95.31%] [G loss: 11.584881]\n",
      "132 [D loss: 0.747299, acc.: 95.31%] [G loss: 12.983484]\n",
      "133 [D loss: 1.679035, acc.: 87.50%] [G loss: 13.277979]\n",
      "134 [D loss: 1.491277, acc.: 89.06%] [G loss: 14.824152]\n",
      "135 [D loss: 0.003296, acc.: 100.00%] [G loss: 14.466408]\n",
      "136 [D loss: 0.147662, acc.: 96.88%] [G loss: 14.250307]\n",
      "137 [D loss: 1.664785, acc.: 78.12%] [G loss: 13.096177]\n",
      "138 [D loss: 1.399880, acc.: 87.50%] [G loss: 12.077150]\n",
      "139 [D loss: 1.246506, acc.: 92.19%] [G loss: 11.584898]\n",
      "140 [D loss: 1.496847, acc.: 90.62%] [G loss: 15.183987]\n",
      "141 [D loss: 0.922969, acc.: 93.75%] [G loss: 14.672276]\n",
      "142 [D loss: 0.145146, acc.: 96.88%] [G loss: 14.618505]\n",
      "143 [D loss: 0.124974, acc.: 96.88%] [G loss: 15.864819]\n",
      "144 [D loss: 0.058175, acc.: 95.31%] [G loss: 15.753260]\n",
      "145 [D loss: 0.001940, acc.: 100.00%] [G loss: 15.614405]\n",
      "146 [D loss: 0.018856, acc.: 98.44%] [G loss: 16.055317]\n",
      "147 [D loss: 0.148677, acc.: 98.44%] [G loss: 16.118095]\n",
      "148 [D loss: 0.157917, acc.: 98.44%] [G loss: 15.206433]\n",
      "149 [D loss: 0.000057, acc.: 100.00%] [G loss: 14.826648]\n",
      "150 [D loss: 0.469982, acc.: 96.88%] [G loss: 15.506549]\n",
      "151 [D loss: 0.000147, acc.: 100.00%] [G loss: 15.551385]\n",
      "152 [D loss: 0.001328, acc.: 100.00%] [G loss: 15.259509]\n",
      "153 [D loss: 0.371274, acc.: 92.19%] [G loss: 16.118095]\n",
      "154 [D loss: 5.545824, acc.: 57.81%] [G loss: 15.989778]\n",
      "155 [D loss: 1.866638, acc.: 79.69%] [G loss: 12.452484]\n",
      "156 [D loss: 0.785582, acc.: 92.19%] [G loss: 11.479980]\n",
      "157 [D loss: 1.698537, acc.: 89.06%] [G loss: 12.859308]\n",
      "158 [D loss: 0.787720, acc.: 93.75%] [G loss: 14.173277]\n",
      "159 [D loss: 0.747299, acc.: 95.31%] [G loss: 13.599643]\n",
      "160 [D loss: 0.956322, acc.: 93.75%] [G loss: 12.502424]\n",
      "161 [D loss: 0.536911, acc.: 95.31%] [G loss: 13.415073]\n",
      "162 [D loss: 0.256522, acc.: 98.44%] [G loss: 15.083424]\n",
      "163 [D loss: 3.422250, acc.: 60.94%] [G loss: 11.353301]\n",
      "164 [D loss: 2.491739, acc.: 84.38%] [G loss: 11.221879]\n",
      "165 [D loss: 2.224732, acc.: 85.94%] [G loss: 11.616949]\n",
      "166 [D loss: 1.787753, acc.: 87.50%] [G loss: 12.693187]\n",
      "167 [D loss: 1.245499, acc.: 92.19%] [G loss: 11.081212]\n",
      "168 [D loss: 1.014383, acc.: 92.19%] [G loss: 12.817543]\n",
      "169 [D loss: 1.743698, acc.: 89.06%] [G loss: 13.095953]\n",
      "170 [D loss: 1.793420, acc.: 87.50%] [G loss: 13.196110]\n",
      "171 [D loss: 1.460268, acc.: 90.62%] [G loss: 12.440363]\n",
      "172 [D loss: 1.245499, acc.: 92.19%] [G loss: 13.526379]\n",
      "173 [D loss: 1.708361, acc.: 87.50%] [G loss: 13.725170]\n",
      "174 [D loss: 2.356232, acc.: 76.56%] [G loss: 11.081190]\n",
      "175 [D loss: 2.929718, acc.: 81.25%] [G loss: 10.672997]\n",
      "176 [D loss: 1.743698, acc.: 89.06%] [G loss: 12.088572]\n",
      "177 [D loss: 1.494600, acc.: 90.62%] [G loss: 10.073831]\n",
      "178 [D loss: 1.248961, acc.: 92.19%] [G loss: 12.088585]\n",
      "179 [D loss: 1.733796, acc.: 89.06%] [G loss: 12.088573]\n",
      "180 [D loss: 2.108113, acc.: 85.94%] [G loss: 13.821012]\n",
      "181 [D loss: 1.246155, acc.: 92.19%] [G loss: 12.088572]\n",
      "182 [D loss: 1.992798, acc.: 87.50%] [G loss: 10.774155]\n",
      "183 [D loss: 1.494599, acc.: 90.62%] [G loss: 11.636047]\n",
      "184 [D loss: 2.989197, acc.: 81.25%] [G loss: 12.088572]\n",
      "185 [D loss: 3.238297, acc.: 79.69%] [G loss: 12.592262]\n",
      "186 [D loss: 1.420459, acc.: 90.62%] [G loss: 13.945238]\n",
      "187 [D loss: 0.747731, acc.: 95.31%] [G loss: 12.023053]\n",
      "188 [D loss: 0.748411, acc.: 95.31%] [G loss: 12.088572]\n",
      "189 [D loss: 1.462107, acc.: 90.62%] [G loss: 15.002281]\n",
      "190 [D loss: 0.433493, acc.: 96.88%] [G loss: 13.962243]\n",
      "191 [D loss: 3.552363, acc.: 70.31%] [G loss: 11.216265]\n",
      "192 [D loss: 1.650679, acc.: 89.06%] [G loss: 13.930258]\n",
      "193 [D loss: 0.249100, acc.: 98.44%] [G loss: 11.539417]\n",
      "194 [D loss: 1.010874, acc.: 92.19%] [G loss: 12.492754]\n",
      "195 [D loss: 0.377700, acc.: 96.88%] [G loss: 13.049483]\n",
      "196 [D loss: 0.126269, acc.: 96.88%] [G loss: 15.861089]\n",
      "197 [D loss: 0.095511, acc.: 96.88%] [G loss: 15.701654]\n",
      "198 [D loss: 0.300162, acc.: 95.31%] [G loss: 16.118095]\n",
      "199 [D loss: 0.000017, acc.: 100.00%] [G loss: 15.844617]\n",
      "200 [D loss: 0.001701, acc.: 100.00%] [G loss: 16.118095]\n",
      "201 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.614405]\n",
      "202 [D loss: 0.000013, acc.: 100.00%] [G loss: 15.539286]\n",
      "203 [D loss: 0.000111, acc.: 100.00%] [G loss: 14.737720]\n",
      "204 [D loss: 0.108704, acc.: 98.44%] [G loss: 15.914904]\n",
      "205 [D loss: 0.249293, acc.: 98.44%] [G loss: 15.070914]\n",
      "206 [D loss: 0.088359, acc.: 96.88%] [G loss: 14.340887]\n",
      "207 [D loss: 0.444935, acc.: 96.88%] [G loss: 14.815433]\n",
      "208 [D loss: 0.073179, acc.: 96.88%] [G loss: 15.866611]\n",
      "209 [D loss: 1.220672, acc.: 81.25%] [G loss: 14.206312]\n",
      "210 [D loss: 1.160466, acc.: 90.62%] [G loss: 15.066227]\n",
      "211 [D loss: 0.299271, acc.: 96.88%] [G loss: 15.614405]\n",
      "212 [D loss: 0.733229, acc.: 90.62%] [G loss: 13.657022]\n",
      "213 [D loss: 0.344511, acc.: 96.88%] [G loss: 15.110718]\n",
      "214 [D loss: 0.281794, acc.: 96.88%] [G loss: 16.118095]\n",
      "215 [D loss: 0.249259, acc.: 98.44%] [G loss: 16.118095]\n",
      "216 [D loss: 0.023628, acc.: 98.44%] [G loss: 14.772577]\n",
      "217 [D loss: 0.522417, acc.: 92.19%] [G loss: 15.521935]\n",
      "218 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.857817]\n",
      "219 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.938698]\n",
      "220 [D loss: 0.612518, acc.: 95.31%] [G loss: 15.906500]\n",
      "221 [D loss: 0.135512, acc.: 96.88%] [G loss: 16.118095]\n",
      "222 [D loss: 0.176128, acc.: 98.44%] [G loss: 15.897751]\n",
      "223 [D loss: 0.057943, acc.: 98.44%] [G loss: 14.744292]\n",
      "224 [D loss: 0.001579, acc.: 100.00%] [G loss: 14.118874]\n",
      "225 [D loss: 0.000005, acc.: 100.00%] [G loss: 15.517328]\n",
      "226 [D loss: 0.206320, acc.: 98.44%] [G loss: 16.118095]\n",
      "227 [D loss: 0.508191, acc.: 96.88%] [G loss: 15.547593]\n",
      "228 [D loss: 0.180961, acc.: 95.31%] [G loss: 15.344918]\n",
      "229 [D loss: 0.388778, acc.: 93.75%] [G loss: 14.105175]\n",
      "230 [D loss: 0.249100, acc.: 98.44%] [G loss: 14.068093]\n",
      "231 [D loss: 0.662061, acc.: 95.31%] [G loss: 14.408733]\n",
      "232 [D loss: 0.074526, acc.: 98.44%] [G loss: 14.920294]\n",
      "233 [D loss: 0.164859, acc.: 98.44%] [G loss: 15.830038]\n",
      "234 [D loss: 2.261326, acc.: 71.88%] [G loss: 12.088572]\n",
      "235 [D loss: 2.100117, acc.: 85.94%] [G loss: 12.592262]\n",
      "236 [D loss: 0.780228, acc.: 93.75%] [G loss: 14.103333]\n",
      "237 [D loss: 1.212513, acc.: 92.19%] [G loss: 12.592262]\n",
      "238 [D loss: 0.747301, acc.: 95.31%] [G loss: 15.110714]\n",
      "239 [D loss: 0.238274, acc.: 98.44%] [G loss: 14.265795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 [D loss: 0.996405, acc.: 93.75%] [G loss: 14.037325]\n",
      "241 [D loss: 0.778781, acc.: 93.75%] [G loss: 15.110748]\n",
      "242 [D loss: 0.937359, acc.: 85.94%] [G loss: 14.103333]\n",
      "243 [D loss: 0.595234, acc.: 95.31%] [G loss: 12.981807]\n",
      "244 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.607023]\n",
      "245 [D loss: 0.249100, acc.: 98.44%] [G loss: 14.103333]\n",
      "246 [D loss: 0.695234, acc.: 95.31%] [G loss: 13.343109]\n",
      "247 [D loss: 0.498200, acc.: 96.88%] [G loss: 14.103333]\n",
      "248 [D loss: 0.363413, acc.: 96.88%] [G loss: 14.565953]\n",
      "249 [D loss: 0.463868, acc.: 96.88%] [G loss: 14.400276]\n",
      "250 [D loss: 0.498318, acc.: 95.31%] [G loss: 15.614405]\n",
      "251 [D loss: 0.666181, acc.: 95.31%] [G loss: 15.110747]\n",
      "252 [D loss: 0.004942, acc.: 100.00%] [G loss: 15.297179]\n",
      "253 [D loss: 0.068668, acc.: 98.44%] [G loss: 15.322525]\n",
      "254 [D loss: 0.112988, acc.: 96.88%] [G loss: 14.608152]\n",
      "255 [D loss: 0.000002, acc.: 100.00%] [G loss: 14.722887]\n",
      "256 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.472911]\n",
      "257 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.614405]\n",
      "258 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.689498]\n",
      "259 [D loss: 0.491469, acc.: 95.31%] [G loss: 15.618614]\n",
      "260 [D loss: 0.069662, acc.: 95.31%] [G loss: 15.508253]\n",
      "261 [D loss: 0.100377, acc.: 96.88%] [G loss: 16.118095]\n",
      "262 [D loss: 0.067928, acc.: 96.88%] [G loss: 15.614489]\n",
      "263 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.347514]\n",
      "264 [D loss: 0.032535, acc.: 98.44%] [G loss: 16.118095]\n",
      "265 [D loss: 0.000006, acc.: 100.00%] [G loss: 15.755611]\n",
      "266 [D loss: 0.001752, acc.: 100.00%] [G loss: 16.118095]\n",
      "267 [D loss: 0.051891, acc.: 98.44%] [G loss: 16.118095]\n",
      "268 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "269 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614443]\n",
      "270 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.966519]\n",
      "271 [D loss: 0.129058, acc.: 98.44%] [G loss: 16.118095]\n",
      "272 [D loss: 0.000072, acc.: 100.00%] [G loss: 15.484104]\n",
      "273 [D loss: 0.011859, acc.: 100.00%] [G loss: 16.118095]\n",
      "274 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.326904]\n",
      "275 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.000130]\n",
      "276 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.988675]\n",
      "277 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.889135]\n",
      "278 [D loss: 0.001164, acc.: 100.00%] [G loss: 16.118095]\n",
      "279 [D loss: 0.249102, acc.: 98.44%] [G loss: 15.121685]\n",
      "280 [D loss: 0.000015, acc.: 100.00%] [G loss: 16.118095]\n",
      "281 [D loss: 0.249102, acc.: 98.44%] [G loss: 16.118095]\n",
      "282 [D loss: 0.000003, acc.: 100.00%] [G loss: 15.264766]\n",
      "283 [D loss: 0.000018, acc.: 100.00%] [G loss: 15.926390]\n",
      "284 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "285 [D loss: 0.444051, acc.: 96.88%] [G loss: 16.118095]\n",
      "286 [D loss: 0.106353, acc.: 98.44%] [G loss: 15.574159]\n",
      "287 [D loss: 0.045718, acc.: 98.44%] [G loss: 15.623258]\n",
      "288 [D loss: 0.001886, acc.: 100.00%] [G loss: 16.118095]\n",
      "289 [D loss: 0.024707, acc.: 98.44%] [G loss: 16.118095]\n",
      "290 [D loss: 0.740772, acc.: 84.38%] [G loss: 14.607024]\n",
      "291 [D loss: 0.750834, acc.: 95.31%] [G loss: 13.912009]\n",
      "292 [D loss: 0.845048, acc.: 93.75%] [G loss: 15.110714]\n",
      "293 [D loss: 0.176451, acc.: 98.44%] [G loss: 14.649576]\n",
      "294 [D loss: 0.498209, acc.: 96.88%] [G loss: 15.131103]\n",
      "295 [D loss: 0.873573, acc.: 93.75%] [G loss: 14.607023]\n",
      "296 [D loss: 0.498262, acc.: 96.88%] [G loss: 14.931422]\n",
      "297 [D loss: 0.498299, acc.: 96.88%] [G loss: 14.607132]\n",
      "298 [D loss: 1.027977, acc.: 92.19%] [G loss: 14.370787]\n",
      "299 [D loss: 0.249110, acc.: 98.44%] [G loss: 14.791007]\n",
      "300 [D loss: 0.855433, acc.: 93.75%] [G loss: 13.856897]\n",
      "301 [D loss: 0.514631, acc.: 95.31%] [G loss: 15.657078]\n",
      "302 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.637393]\n",
      "303 [D loss: 0.249102, acc.: 98.44%] [G loss: 16.118095]\n",
      "304 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.606930]\n",
      "305 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.169018]\n",
      "306 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "307 [D loss: 0.001576, acc.: 100.00%] [G loss: 16.118095]\n",
      "308 [D loss: 0.000033, acc.: 100.00%] [G loss: 15.110723]\n",
      "309 [D loss: 0.000119, acc.: 100.00%] [G loss: 16.118095]\n",
      "310 [D loss: 0.000035, acc.: 100.00%] [G loss: 15.110715]\n",
      "311 [D loss: 0.017163, acc.: 98.44%] [G loss: 16.118095]\n",
      "312 [D loss: 0.034510, acc.: 98.44%] [G loss: 16.118095]\n",
      "313 [D loss: 0.001776, acc.: 100.00%] [G loss: 15.614498]\n",
      "314 [D loss: 0.165685, acc.: 96.88%] [G loss: 16.118095]\n",
      "315 [D loss: 0.007086, acc.: 100.00%] [G loss: 15.723292]\n",
      "316 [D loss: 0.271865, acc.: 90.62%] [G loss: 14.759165]\n",
      "317 [D loss: 0.000177, acc.: 100.00%] [G loss: 15.110714]\n",
      "318 [D loss: 0.468957, acc.: 95.31%] [G loss: 14.607023]\n",
      "319 [D loss: 0.304808, acc.: 95.31%] [G loss: 15.654785]\n",
      "320 [D loss: 0.000151, acc.: 100.00%] [G loss: 15.614405]\n",
      "321 [D loss: 0.249273, acc.: 98.44%] [G loss: 14.995404]\n",
      "322 [D loss: 0.000001, acc.: 100.00%] [G loss: 14.619303]\n",
      "323 [D loss: 0.985696, acc.: 93.75%] [G loss: 15.110722]\n",
      "324 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.653417]\n",
      "325 [D loss: 0.249106, acc.: 98.44%] [G loss: 16.118095]\n",
      "326 [D loss: 0.453932, acc.: 96.88%] [G loss: 16.118095]\n",
      "327 [D loss: 0.005206, acc.: 100.00%] [G loss: 16.118095]\n",
      "328 [D loss: 0.263176, acc.: 96.88%] [G loss: 15.112713]\n",
      "329 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614424]\n",
      "330 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "331 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "332 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "333 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "334 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "335 [D loss: 0.011480, acc.: 98.44%] [G loss: 16.118095]\n",
      "336 [D loss: 0.013797, acc.: 98.44%] [G loss: 16.046917]\n",
      "337 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "338 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.834669]\n",
      "339 [D loss: 0.268963, acc.: 96.88%] [G loss: 16.118095]\n",
      "340 [D loss: 0.198763, acc.: 96.88%] [G loss: 14.906931]\n",
      "341 [D loss: 0.152700, acc.: 98.44%] [G loss: 14.744805]\n",
      "342 [D loss: 0.433900, acc.: 96.88%] [G loss: 16.015135]\n",
      "343 [D loss: 0.004417, acc.: 100.00%] [G loss: 16.118095]\n",
      "344 [D loss: 0.749330, acc.: 89.06%] [G loss: 15.264583]\n",
      "345 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.614469]\n",
      "346 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "347 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.741265]\n",
      "348 [D loss: 0.000079, acc.: 100.00%] [G loss: 15.614405]\n",
      "349 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "350 [D loss: 0.249100, acc.: 98.44%] [G loss: 14.607751]\n",
      "351 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.242062]\n",
      "352 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.620243]\n",
      "353 [D loss: 0.064537, acc.: 98.44%] [G loss: 15.981348]\n",
      "354 [D loss: 0.000026, acc.: 100.00%] [G loss: 15.614635]\n",
      "355 [D loss: 0.557108, acc.: 95.31%] [G loss: 15.614405]\n",
      "356 [D loss: 0.000694, acc.: 100.00%] [G loss: 15.425421]\n",
      "357 [D loss: 0.098361, acc.: 98.44%] [G loss: 15.499907]\n",
      "358 [D loss: 0.045042, acc.: 96.88%] [G loss: 13.622303]\n",
      "359 [D loss: 0.000039, acc.: 100.00%] [G loss: 15.842565]\n",
      "360 [D loss: 0.046878, acc.: 98.44%] [G loss: 15.111054]\n",
      "361 [D loss: 0.249374, acc.: 98.44%] [G loss: 15.967946]\n",
      "362 [D loss: 0.000186, acc.: 100.00%] [G loss: 15.838255]\n",
      "363 [D loss: 0.007705, acc.: 100.00%] [G loss: 15.549158]\n",
      "364 [D loss: 0.249104, acc.: 98.44%] [G loss: 16.118095]\n",
      "365 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.607230]\n",
      "366 [D loss: 0.081535, acc.: 96.88%] [G loss: 16.118095]\n",
      "367 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "368 [D loss: 0.146423, acc.: 96.88%] [G loss: 15.110715]\n",
      "369 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614458]\n",
      "370 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "371 [D loss: 0.498200, acc.: 96.88%] [G loss: 14.221871]\n",
      "372 [D loss: 0.498202, acc.: 96.88%] [G loss: 15.168874]\n",
      "373 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.110714]\n",
      "374 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "375 [D loss: 0.000003, acc.: 100.00%] [G loss: 14.607506]\n",
      "376 [D loss: 0.398457, acc.: 96.88%] [G loss: 15.110716]\n",
      "377 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "378 [D loss: 0.000019, acc.: 100.00%] [G loss: 15.614536]\n",
      "379 [D loss: 0.106525, acc.: 96.88%] [G loss: 16.118095]\n",
      "380 [D loss: 0.089793, acc.: 98.44%] [G loss: 16.118095]\n",
      "381 [D loss: 0.000024, acc.: 100.00%] [G loss: 16.074474]\n",
      "382 [D loss: 0.249101, acc.: 98.44%] [G loss: 15.615259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383 [D loss: 0.011084, acc.: 98.44%] [G loss: 15.614405]\n",
      "384 [D loss: 0.078888, acc.: 98.44%] [G loss: 15.043044]\n",
      "385 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "386 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "387 [D loss: 0.000006, acc.: 100.00%] [G loss: 15.614405]\n",
      "388 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "389 [D loss: 0.473055, acc.: 96.88%] [G loss: 15.177720]\n",
      "390 [D loss: 0.001785, acc.: 100.00%] [G loss: 15.110859]\n",
      "391 [D loss: 0.019755, acc.: 100.00%] [G loss: 16.118095]\n",
      "392 [D loss: 0.247104, acc.: 96.88%] [G loss: 16.118095]\n",
      "393 [D loss: 0.957541, acc.: 79.69%] [G loss: 14.607023]\n",
      "394 [D loss: 1.245516, acc.: 92.19%] [G loss: 14.103333]\n",
      "395 [D loss: 1.135025, acc.: 92.19%] [G loss: 14.560685]\n",
      "396 [D loss: 0.000034, acc.: 100.00%] [G loss: 15.614405]\n",
      "397 [D loss: 0.251270, acc.: 98.44%] [G loss: 13.729662]\n",
      "398 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "399 [D loss: 1.004433, acc.: 92.19%] [G loss: 14.260523]\n",
      "400 [D loss: 0.498202, acc.: 96.88%] [G loss: 15.614405]\n",
      "401 [D loss: 0.692700, acc.: 92.19%] [G loss: 14.608549]\n",
      "402 [D loss: 0.005710, acc.: 100.00%] [G loss: 14.607023]\n",
      "403 [D loss: 0.996399, acc.: 93.75%] [G loss: 14.127962]\n",
      "404 [D loss: 0.249100, acc.: 98.44%] [G loss: 14.487604]\n",
      "405 [D loss: 0.249101, acc.: 98.44%] [G loss: 16.118095]\n",
      "406 [D loss: 0.736469, acc.: 95.31%] [G loss: 15.059802]\n",
      "407 [D loss: 0.023271, acc.: 98.44%] [G loss: 15.326860]\n",
      "408 [D loss: 0.249100, acc.: 98.44%] [G loss: 13.162819]\n",
      "409 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "410 [D loss: 0.498201, acc.: 96.88%] [G loss: 15.532208]\n",
      "411 [D loss: 0.481046, acc.: 96.88%] [G loss: 16.080786]\n",
      "412 [D loss: 0.000799, acc.: 100.00%] [G loss: 15.614405]\n",
      "413 [D loss: 0.352738, acc.: 96.88%] [G loss: 16.118095]\n",
      "414 [D loss: 0.001848, acc.: 100.00%] [G loss: 16.118095]\n",
      "415 [D loss: 0.026297, acc.: 96.88%] [G loss: 15.129127]\n",
      "416 [D loss: 0.161080, acc.: 95.31%] [G loss: 15.368567]\n",
      "417 [D loss: 0.013322, acc.: 98.44%] [G loss: 15.984476]\n",
      "418 [D loss: 0.000758, acc.: 100.00%] [G loss: 15.614447]\n",
      "419 [D loss: 0.000393, acc.: 100.00%] [G loss: 16.118095]\n",
      "420 [D loss: 0.016675, acc.: 98.44%] [G loss: 15.766994]\n",
      "421 [D loss: 0.000008, acc.: 100.00%] [G loss: 15.423813]\n",
      "422 [D loss: 0.219521, acc.: 98.44%] [G loss: 16.118095]\n",
      "423 [D loss: 0.000236, acc.: 100.00%] [G loss: 16.118095]\n",
      "424 [D loss: 0.012615, acc.: 98.44%] [G loss: 15.614405]\n",
      "425 [D loss: 0.198192, acc.: 98.44%] [G loss: 15.712715]\n",
      "426 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.691072]\n",
      "427 [D loss: 0.020229, acc.: 98.44%] [G loss: 16.085106]\n",
      "428 [D loss: 0.092905, acc.: 98.44%] [G loss: 15.614824]\n",
      "429 [D loss: 0.010650, acc.: 100.00%] [G loss: 16.118095]\n",
      "430 [D loss: 0.050464, acc.: 96.88%] [G loss: 15.614405]\n",
      "431 [D loss: 0.487369, acc.: 96.88%] [G loss: 16.118095]\n",
      "432 [D loss: 0.004576, acc.: 100.00%] [G loss: 15.614414]\n",
      "433 [D loss: 0.016195, acc.: 98.44%] [G loss: 16.118095]\n",
      "434 [D loss: 0.000028, acc.: 100.00%] [G loss: 15.536030]\n",
      "435 [D loss: 0.058496, acc.: 98.44%] [G loss: 16.118095]\n",
      "436 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "437 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "438 [D loss: 0.339250, acc.: 96.88%] [G loss: 16.118095]\n",
      "439 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.995883]\n",
      "440 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "441 [D loss: 0.052398, acc.: 98.44%] [G loss: 16.118095]\n",
      "442 [D loss: 0.002475, acc.: 100.00%] [G loss: 15.733660]\n",
      "443 [D loss: 0.000026, acc.: 100.00%] [G loss: 15.905632]\n",
      "444 [D loss: 0.000156, acc.: 100.00%] [G loss: 16.118095]\n",
      "445 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "446 [D loss: 0.000281, acc.: 100.00%] [G loss: 16.118095]\n",
      "447 [D loss: 0.001597, acc.: 100.00%] [G loss: 16.118095]\n",
      "448 [D loss: 0.001822, acc.: 100.00%] [G loss: 16.118095]\n",
      "449 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "450 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.712589]\n",
      "451 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "452 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "453 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "454 [D loss: 0.000016, acc.: 100.00%] [G loss: 16.118095]\n",
      "455 [D loss: 0.105080, acc.: 98.44%] [G loss: 16.118095]\n",
      "456 [D loss: 0.236259, acc.: 95.31%] [G loss: 16.118095]\n",
      "457 [D loss: 0.000016, acc.: 100.00%] [G loss: 15.614405]\n",
      "458 [D loss: 0.201529, acc.: 98.44%] [G loss: 15.614411]\n",
      "459 [D loss: 0.316305, acc.: 96.88%] [G loss: 15.847892]\n",
      "460 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "461 [D loss: 0.168897, acc.: 98.44%] [G loss: 16.118095]\n",
      "462 [D loss: 0.001496, acc.: 100.00%] [G loss: 16.118095]\n",
      "463 [D loss: 0.024652, acc.: 98.44%] [G loss: 16.118095]\n",
      "464 [D loss: 0.133468, acc.: 98.44%] [G loss: 16.118095]\n",
      "465 [D loss: 0.000078, acc.: 100.00%] [G loss: 16.118095]\n",
      "466 [D loss: 0.965768, acc.: 84.38%] [G loss: 14.990021]\n",
      "467 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.922476]\n",
      "468 [D loss: 0.713130, acc.: 93.75%] [G loss: 15.614405]\n",
      "469 [D loss: 0.760396, acc.: 93.75%] [G loss: 16.118095]\n",
      "470 [D loss: 2.201282, acc.: 81.25%] [G loss: 16.118095]\n",
      "471 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.110714]\n",
      "472 [D loss: 0.634157, acc.: 95.31%] [G loss: 15.326684]\n",
      "473 [D loss: 0.293072, acc.: 96.88%] [G loss: 15.110714]\n",
      "474 [D loss: 0.446134, acc.: 96.88%] [G loss: 15.115396]\n",
      "475 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "476 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110781]\n",
      "477 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "478 [D loss: 0.100899, acc.: 98.44%] [G loss: 16.118095]\n",
      "479 [D loss: 0.000014, acc.: 100.00%] [G loss: 16.118095]\n",
      "480 [D loss: 0.256455, acc.: 98.44%] [G loss: 16.118095]\n",
      "481 [D loss: 0.000047, acc.: 100.00%] [G loss: 16.118095]\n",
      "482 [D loss: 0.000130, acc.: 100.00%] [G loss: 15.614405]\n",
      "483 [D loss: 0.062746, acc.: 98.44%] [G loss: 16.118095]\n",
      "484 [D loss: 0.018685, acc.: 98.44%] [G loss: 16.118095]\n",
      "485 [D loss: 0.001255, acc.: 100.00%] [G loss: 16.118095]\n",
      "486 [D loss: 0.000008, acc.: 100.00%] [G loss: 15.186485]\n",
      "487 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "488 [D loss: 0.000010, acc.: 100.00%] [G loss: 16.118095]\n",
      "489 [D loss: 0.000003, acc.: 100.00%] [G loss: 15.645089]\n",
      "490 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "491 [D loss: 0.000005, acc.: 100.00%] [G loss: 15.660791]\n",
      "492 [D loss: 0.227439, acc.: 98.44%] [G loss: 16.118095]\n",
      "493 [D loss: 0.001991, acc.: 100.00%] [G loss: 16.118095]\n",
      "494 [D loss: 0.000178, acc.: 100.00%] [G loss: 16.118095]\n",
      "495 [D loss: 0.000221, acc.: 100.00%] [G loss: 16.118095]\n",
      "496 [D loss: 0.000050, acc.: 100.00%] [G loss: 16.118095]\n",
      "497 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "498 [D loss: 0.002391, acc.: 100.00%] [G loss: 15.264679]\n",
      "499 [D loss: 0.014718, acc.: 98.44%] [G loss: 16.118095]\n",
      "500 [D loss: 0.047162, acc.: 98.44%] [G loss: 16.118095]\n",
      "501 [D loss: 0.414832, acc.: 96.88%] [G loss: 16.118095]\n",
      "502 [D loss: 0.044569, acc.: 98.44%] [G loss: 15.770424]\n",
      "503 [D loss: 0.000022, acc.: 100.00%] [G loss: 15.614414]\n",
      "504 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.044871]\n",
      "505 [D loss: 0.000004, acc.: 100.00%] [G loss: 15.636059]\n",
      "506 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.622165]\n",
      "507 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.948330]\n",
      "508 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "509 [D loss: 0.006101, acc.: 100.00%] [G loss: 15.622102]\n",
      "510 [D loss: 0.000015, acc.: 100.00%] [G loss: 16.118095]\n",
      "511 [D loss: 0.000034, acc.: 100.00%] [G loss: 15.621460]\n",
      "512 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.777353]\n",
      "513 [D loss: 0.000406, acc.: 100.00%] [G loss: 16.118095]\n",
      "514 [D loss: 0.000025, acc.: 100.00%] [G loss: 16.118095]\n",
      "515 [D loss: 0.066865, acc.: 98.44%] [G loss: 16.118095]\n",
      "516 [D loss: 0.132738, acc.: 95.31%] [G loss: 15.614405]\n",
      "517 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.072525]\n",
      "518 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "519 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "520 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "521 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.121739]\n",
      "522 [D loss: 0.263433, acc.: 96.88%] [G loss: 15.652122]\n",
      "523 [D loss: 0.089902, acc.: 98.44%] [G loss: 16.118095]\n",
      "524 [D loss: 0.412270, acc.: 95.31%] [G loss: 15.897881]\n",
      "525 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526 [D loss: 0.084741, acc.: 98.44%] [G loss: 16.118095]\n",
      "527 [D loss: 0.000026, acc.: 100.00%] [G loss: 15.654880]\n",
      "528 [D loss: 0.095068, acc.: 96.88%] [G loss: 16.118095]\n",
      "529 [D loss: 0.794367, acc.: 82.81%] [G loss: 14.607023]\n",
      "530 [D loss: 0.504196, acc.: 96.88%] [G loss: 15.110714]\n",
      "531 [D loss: 0.498200, acc.: 96.88%] [G loss: 14.981442]\n",
      "532 [D loss: 0.996399, acc.: 93.75%] [G loss: 14.607024]\n",
      "533 [D loss: 0.747299, acc.: 95.31%] [G loss: 15.110714]\n",
      "534 [D loss: 0.000242, acc.: 100.00%] [G loss: 14.787120]\n",
      "535 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110714]\n",
      "536 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "537 [D loss: 0.249100, acc.: 98.44%] [G loss: 14.166262]\n",
      "538 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.111763]\n",
      "539 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "540 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.614407]\n",
      "541 [D loss: 0.569280, acc.: 95.31%] [G loss: 15.614405]\n",
      "542 [D loss: 0.249637, acc.: 98.44%] [G loss: 15.110714]\n",
      "543 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110840]\n",
      "544 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.614405]\n",
      "545 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "546 [D loss: 0.301011, acc.: 96.88%] [G loss: 15.110758]\n",
      "547 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.663873]\n",
      "548 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "549 [D loss: 0.023236, acc.: 98.44%] [G loss: 15.614405]\n",
      "550 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "551 [D loss: 0.089314, acc.: 98.44%] [G loss: 14.800861]\n",
      "552 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "553 [D loss: 0.467795, acc.: 96.88%] [G loss: 15.614405]\n",
      "554 [D loss: 0.250441, acc.: 98.44%] [G loss: 16.118095]\n",
      "555 [D loss: 0.300686, acc.: 96.88%] [G loss: 15.614405]\n",
      "556 [D loss: 0.106221, acc.: 98.44%] [G loss: 15.614513]\n",
      "557 [D loss: 1.111693, acc.: 90.62%] [G loss: 15.110721]\n",
      "558 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.607024]\n",
      "559 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "560 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.328234]\n",
      "561 [D loss: 0.463868, acc.: 96.88%] [G loss: 15.722921]\n",
      "562 [D loss: 0.060959, acc.: 98.44%] [G loss: 16.118095]\n",
      "563 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "564 [D loss: 0.726980, acc.: 89.06%] [G loss: 15.628638]\n",
      "565 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "566 [D loss: 0.367732, acc.: 96.88%] [G loss: 16.118095]\n",
      "567 [D loss: 0.556259, acc.: 93.75%] [G loss: 15.248617]\n",
      "568 [D loss: 0.694383, acc.: 92.19%] [G loss: 15.614405]\n",
      "569 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "570 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.498697]\n",
      "571 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "572 [D loss: 0.249100, acc.: 98.44%] [G loss: 14.607024]\n",
      "573 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110746]\n",
      "574 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.111034]\n",
      "575 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "576 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "577 [D loss: 0.498200, acc.: 96.88%] [G loss: 14.928040]\n",
      "578 [D loss: 0.498200, acc.: 96.88%] [G loss: 14.622763]\n",
      "579 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.787860]\n",
      "580 [D loss: 0.011638, acc.: 98.44%] [G loss: 16.118095]\n",
      "581 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "582 [D loss: 0.467795, acc.: 96.88%] [G loss: 16.118095]\n",
      "583 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.096424]\n",
      "584 [D loss: 0.000214, acc.: 100.00%] [G loss: 16.118095]\n",
      "585 [D loss: 0.365006, acc.: 96.88%] [G loss: 16.118095]\n",
      "586 [D loss: 0.000025, acc.: 100.00%] [G loss: 15.611895]\n",
      "587 [D loss: 0.016777, acc.: 100.00%] [G loss: 16.118095]\n",
      "588 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "589 [D loss: 0.221104, acc.: 98.44%] [G loss: 16.118095]\n",
      "590 [D loss: 0.000751, acc.: 100.00%] [G loss: 16.118095]\n",
      "591 [D loss: 0.035714, acc.: 98.44%] [G loss: 16.118095]\n",
      "592 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "593 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "594 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "595 [D loss: 0.046944, acc.: 98.44%] [G loss: 15.812462]\n",
      "596 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "597 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "598 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "599 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "600 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "601 [D loss: 0.000085, acc.: 100.00%] [G loss: 15.614405]\n",
      "602 [D loss: 0.000078, acc.: 100.00%] [G loss: 16.118095]\n",
      "603 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "604 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.886467]\n",
      "605 [D loss: 0.000068, acc.: 100.00%] [G loss: 16.118095]\n",
      "606 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "607 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "608 [D loss: 0.000005, acc.: 100.00%] [G loss: 16.118095]\n",
      "609 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.912697]\n",
      "610 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "611 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "612 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "613 [D loss: 0.000065, acc.: 100.00%] [G loss: 16.118095]\n",
      "614 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "615 [D loss: 0.000057, acc.: 100.00%] [G loss: 16.118095]\n",
      "616 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.115742]\n",
      "617 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614408]\n",
      "618 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.614405]\n",
      "619 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "620 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.255083]\n",
      "621 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.092152]\n",
      "622 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "623 [D loss: 0.162405, acc.: 98.44%] [G loss: 16.118095]\n",
      "624 [D loss: 0.000008, acc.: 100.00%] [G loss: 15.614405]\n",
      "625 [D loss: 0.001749, acc.: 100.00%] [G loss: 16.118095]\n",
      "626 [D loss: 0.046551, acc.: 96.88%] [G loss: 15.614405]\n",
      "627 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614503]\n",
      "628 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "629 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.952765]\n",
      "630 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "631 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.156105]\n",
      "632 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "633 [D loss: 0.164042, acc.: 96.88%] [G loss: 16.118095]\n",
      "634 [D loss: 0.065778, acc.: 98.44%] [G loss: 16.118095]\n",
      "635 [D loss: 0.000440, acc.: 100.00%] [G loss: 15.590733]\n",
      "636 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "637 [D loss: 0.176285, acc.: 98.44%] [G loss: 16.118095]\n",
      "638 [D loss: 0.082554, acc.: 98.44%] [G loss: 15.499477]\n",
      "639 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.127502]\n",
      "640 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.055538]\n",
      "641 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "642 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "643 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.531329]\n",
      "644 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "645 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110719]\n",
      "646 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110714]\n",
      "647 [D loss: 0.203093, acc.: 98.44%] [G loss: 16.118095]\n",
      "648 [D loss: 0.004351, acc.: 100.00%] [G loss: 16.118095]\n",
      "649 [D loss: 0.232282, acc.: 95.31%] [G loss: 16.118095]\n",
      "650 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "651 [D loss: 0.073048, acc.: 98.44%] [G loss: 16.118095]\n",
      "652 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "653 [D loss: 0.012537, acc.: 98.44%] [G loss: 15.697556]\n",
      "654 [D loss: 0.174070, acc.: 98.44%] [G loss: 16.118095]\n",
      "655 [D loss: 0.238350, acc.: 98.44%] [G loss: 15.624333]\n",
      "656 [D loss: 0.300968, acc.: 92.19%] [G loss: 16.118095]\n",
      "657 [D loss: 0.000168, acc.: 100.00%] [G loss: 16.118095]\n",
      "658 [D loss: 0.000034, acc.: 100.00%] [G loss: 15.913947]\n",
      "659 [D loss: 0.000168, acc.: 100.00%] [G loss: 15.614416]\n",
      "660 [D loss: 0.000195, acc.: 100.00%] [G loss: 16.118095]\n",
      "661 [D loss: 0.000084, acc.: 100.00%] [G loss: 16.118095]\n",
      "662 [D loss: 0.000038, acc.: 100.00%] [G loss: 15.244571]\n",
      "663 [D loss: 0.000207, acc.: 100.00%] [G loss: 16.118095]\n",
      "664 [D loss: 0.000034, acc.: 100.00%] [G loss: 16.118095]\n",
      "665 [D loss: 0.000162, acc.: 100.00%] [G loss: 16.118095]\n",
      "666 [D loss: 0.000058, acc.: 100.00%] [G loss: 15.666588]\n",
      "667 [D loss: 0.000245, acc.: 100.00%] [G loss: 15.192240]\n",
      "668 [D loss: 0.000056, acc.: 100.00%] [G loss: 16.018436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669 [D loss: 0.000191, acc.: 100.00%] [G loss: 16.118095]\n",
      "670 [D loss: 0.000933, acc.: 100.00%] [G loss: 16.118095]\n",
      "671 [D loss: 0.000064, acc.: 100.00%] [G loss: 15.518837]\n",
      "672 [D loss: 0.000109, acc.: 100.00%] [G loss: 15.614435]\n",
      "673 [D loss: 0.028033, acc.: 98.44%] [G loss: 15.903028]\n",
      "674 [D loss: 0.001378, acc.: 100.00%] [G loss: 16.118095]\n",
      "675 [D loss: 0.007369, acc.: 100.00%] [G loss: 16.118095]\n",
      "676 [D loss: 0.000138, acc.: 100.00%] [G loss: 15.915470]\n",
      "677 [D loss: 0.000075, acc.: 100.00%] [G loss: 15.580343]\n",
      "678 [D loss: 0.000053, acc.: 100.00%] [G loss: 15.663387]\n",
      "679 [D loss: 0.000031, acc.: 100.00%] [G loss: 14.623150]\n",
      "680 [D loss: 0.209157, acc.: 98.44%] [G loss: 15.617109]\n",
      "681 [D loss: 0.347751, acc.: 95.31%] [G loss: 15.614779]\n",
      "682 [D loss: 0.000005, acc.: 100.00%] [G loss: 15.112358]\n",
      "683 [D loss: 0.000408, acc.: 100.00%] [G loss: 15.844849]\n",
      "684 [D loss: 0.001813, acc.: 100.00%] [G loss: 16.118095]\n",
      "685 [D loss: 0.000009, acc.: 100.00%] [G loss: 15.923180]\n",
      "686 [D loss: 0.000051, acc.: 100.00%] [G loss: 16.118095]\n",
      "687 [D loss: 0.118177, acc.: 98.44%] [G loss: 16.111490]\n",
      "688 [D loss: 0.046683, acc.: 98.44%] [G loss: 16.118095]\n",
      "689 [D loss: 0.000069, acc.: 100.00%] [G loss: 16.118095]\n",
      "690 [D loss: 0.149560, acc.: 98.44%] [G loss: 15.902372]\n",
      "691 [D loss: 0.161854, acc.: 92.19%] [G loss: 15.614405]\n",
      "692 [D loss: 0.157659, acc.: 98.44%] [G loss: 15.614405]\n",
      "693 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "694 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "695 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.618546]\n",
      "696 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.083603]\n",
      "697 [D loss: 0.278791, acc.: 96.88%] [G loss: 16.118095]\n",
      "698 [D loss: 0.000119, acc.: 100.00%] [G loss: 16.118095]\n",
      "699 [D loss: 0.007012, acc.: 100.00%] [G loss: 16.118095]\n",
      "700 [D loss: 0.000270, acc.: 100.00%] [G loss: 16.118095]\n",
      "701 [D loss: 0.000343, acc.: 100.00%] [G loss: 16.118095]\n",
      "702 [D loss: 0.000072, acc.: 100.00%] [G loss: 16.118095]\n",
      "703 [D loss: 0.000020, acc.: 100.00%] [G loss: 16.118095]\n",
      "704 [D loss: 0.000113, acc.: 100.00%] [G loss: 16.118095]\n",
      "705 [D loss: 0.004548, acc.: 100.00%] [G loss: 16.027246]\n",
      "706 [D loss: 0.177460, acc.: 98.44%] [G loss: 16.118095]\n",
      "707 [D loss: 0.000019, acc.: 100.00%] [G loss: 16.118095]\n",
      "708 [D loss: 0.000527, acc.: 100.00%] [G loss: 15.619234]\n",
      "709 [D loss: 0.000907, acc.: 100.00%] [G loss: 16.118095]\n",
      "710 [D loss: 0.000086, acc.: 100.00%] [G loss: 16.118095]\n",
      "711 [D loss: 0.233693, acc.: 98.44%] [G loss: 15.696681]\n",
      "712 [D loss: 0.001697, acc.: 100.00%] [G loss: 15.453761]\n",
      "713 [D loss: 0.000138, acc.: 100.00%] [G loss: 16.118095]\n",
      "714 [D loss: 0.000443, acc.: 100.00%] [G loss: 16.102190]\n",
      "715 [D loss: 0.000265, acc.: 100.00%] [G loss: 16.104847]\n",
      "716 [D loss: 0.000208, acc.: 100.00%] [G loss: 15.775072]\n",
      "717 [D loss: 0.006866, acc.: 100.00%] [G loss: 15.795794]\n",
      "718 [D loss: 0.000278, acc.: 100.00%] [G loss: 16.101494]\n",
      "719 [D loss: 0.026583, acc.: 100.00%] [G loss: 16.118095]\n",
      "720 [D loss: 0.249114, acc.: 98.44%] [G loss: 16.118095]\n",
      "721 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "722 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "723 [D loss: 0.004832, acc.: 100.00%] [G loss: 16.118095]\n",
      "724 [D loss: 0.000005, acc.: 100.00%] [G loss: 15.702738]\n",
      "725 [D loss: 0.076570, acc.: 98.44%] [G loss: 16.118095]\n",
      "726 [D loss: 0.028917, acc.: 96.88%] [G loss: 16.118095]\n",
      "727 [D loss: 0.121521, acc.: 98.44%] [G loss: 16.118095]\n",
      "728 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "729 [D loss: 0.000006, acc.: 100.00%] [G loss: 16.118095]\n",
      "730 [D loss: 0.000033, acc.: 100.00%] [G loss: 15.266222]\n",
      "731 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "732 [D loss: 0.249230, acc.: 98.44%] [G loss: 16.118095]\n",
      "733 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "734 [D loss: 0.000121, acc.: 100.00%] [G loss: 15.614533]\n",
      "735 [D loss: 0.000232, acc.: 100.00%] [G loss: 15.824861]\n",
      "736 [D loss: 0.000114, acc.: 100.00%] [G loss: 15.335260]\n",
      "737 [D loss: 0.000320, acc.: 100.00%] [G loss: 16.118095]\n",
      "738 [D loss: 0.000090, acc.: 100.00%] [G loss: 15.920389]\n",
      "739 [D loss: 0.000035, acc.: 100.00%] [G loss: 16.118095]\n",
      "740 [D loss: 0.000058, acc.: 100.00%] [G loss: 16.118095]\n",
      "741 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "742 [D loss: 0.000020, acc.: 100.00%] [G loss: 15.519538]\n",
      "743 [D loss: 0.000063, acc.: 100.00%] [G loss: 16.118095]\n",
      "744 [D loss: 0.000155, acc.: 100.00%] [G loss: 16.118095]\n",
      "745 [D loss: 0.000005, acc.: 100.00%] [G loss: 15.614405]\n",
      "746 [D loss: 0.000054, acc.: 100.00%] [G loss: 15.614744]\n",
      "747 [D loss: 0.000076, acc.: 100.00%] [G loss: 15.614409]\n",
      "748 [D loss: 0.003397, acc.: 100.00%] [G loss: 16.118095]\n",
      "749 [D loss: 0.000010, acc.: 100.00%] [G loss: 16.118095]\n",
      "750 [D loss: 0.000220, acc.: 100.00%] [G loss: 16.118095]\n",
      "751 [D loss: 0.000024, acc.: 100.00%] [G loss: 16.070318]\n",
      "752 [D loss: 0.000023, acc.: 100.00%] [G loss: 16.118095]\n",
      "753 [D loss: 0.000224, acc.: 100.00%] [G loss: 16.118095]\n",
      "754 [D loss: 0.000172, acc.: 100.00%] [G loss: 16.118095]\n",
      "755 [D loss: 0.040769, acc.: 98.44%] [G loss: 16.118095]\n",
      "756 [D loss: 0.097062, acc.: 95.31%] [G loss: 16.011250]\n",
      "757 [D loss: 0.144630, acc.: 96.88%] [G loss: 16.118095]\n",
      "758 [D loss: 0.000047, acc.: 100.00%] [G loss: 15.968439]\n",
      "759 [D loss: 0.007380, acc.: 100.00%] [G loss: 16.118095]\n",
      "760 [D loss: 0.017560, acc.: 100.00%] [G loss: 15.807516]\n",
      "761 [D loss: 0.076115, acc.: 98.44%] [G loss: 16.118095]\n",
      "762 [D loss: 0.000022, acc.: 100.00%] [G loss: 16.118095]\n",
      "763 [D loss: 0.000050, acc.: 100.00%] [G loss: 16.118095]\n",
      "764 [D loss: 0.000017, acc.: 100.00%] [G loss: 16.118095]\n",
      "765 [D loss: 0.005533, acc.: 100.00%] [G loss: 16.118095]\n",
      "766 [D loss: 0.000057, acc.: 100.00%] [G loss: 16.118095]\n",
      "767 [D loss: 0.000353, acc.: 100.00%] [G loss: 16.118095]\n",
      "768 [D loss: 0.003091, acc.: 100.00%] [G loss: 16.118095]\n",
      "769 [D loss: 0.000142, acc.: 100.00%] [G loss: 16.118095]\n",
      "770 [D loss: 0.000014, acc.: 100.00%] [G loss: 15.614406]\n",
      "771 [D loss: 0.000032, acc.: 100.00%] [G loss: 16.118095]\n",
      "772 [D loss: 0.000425, acc.: 100.00%] [G loss: 16.118095]\n",
      "773 [D loss: 0.000188, acc.: 100.00%] [G loss: 16.118095]\n",
      "774 [D loss: 0.000094, acc.: 100.00%] [G loss: 16.118095]\n",
      "775 [D loss: 0.000642, acc.: 100.00%] [G loss: 16.118095]\n",
      "776 [D loss: 0.000484, acc.: 100.00%] [G loss: 16.118095]\n",
      "777 [D loss: 0.249105, acc.: 98.44%] [G loss: 15.809204]\n",
      "778 [D loss: 0.000007, acc.: 100.00%] [G loss: 16.118095]\n",
      "779 [D loss: 0.000365, acc.: 100.00%] [G loss: 15.614405]\n",
      "780 [D loss: 0.000002, acc.: 100.00%] [G loss: 16.118095]\n",
      "781 [D loss: 0.094200, acc.: 98.44%] [G loss: 15.843887]\n",
      "782 [D loss: 0.005477, acc.: 100.00%] [G loss: 16.118095]\n",
      "783 [D loss: 0.046773, acc.: 98.44%] [G loss: 16.118095]\n",
      "784 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.760076]\n",
      "785 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "786 [D loss: 0.001882, acc.: 100.00%] [G loss: 16.118095]\n",
      "787 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.902923]\n",
      "788 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "789 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.825465]\n",
      "790 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.633671]\n",
      "791 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.621465]\n",
      "792 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "793 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "794 [D loss: 0.281648, acc.: 96.88%] [G loss: 16.118095]\n",
      "795 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "796 [D loss: 0.003252, acc.: 100.00%] [G loss: 16.118095]\n",
      "797 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "798 [D loss: 0.000083, acc.: 100.00%] [G loss: 16.118095]\n",
      "799 [D loss: 0.135218, acc.: 98.44%] [G loss: 15.614406]\n",
      "800 [D loss: 0.005272, acc.: 100.00%] [G loss: 15.359335]\n",
      "801 [D loss: 0.247078, acc.: 89.06%] [G loss: 16.118095]\n",
      "802 [D loss: 0.254626, acc.: 98.44%] [G loss: 14.612427]\n",
      "803 [D loss: 0.007040, acc.: 100.00%] [G loss: 15.614405]\n",
      "804 [D loss: 0.449902, acc.: 96.88%] [G loss: 15.614405]\n",
      "805 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "806 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.475145]\n",
      "807 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "808 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110714]\n",
      "809 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "810 [D loss: 0.000028, acc.: 100.00%] [G loss: 15.614405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "812 [D loss: 0.157000, acc.: 98.44%] [G loss: 15.614405]\n",
      "813 [D loss: 0.000806, acc.: 100.00%] [G loss: 16.118095]\n",
      "814 [D loss: 0.296486, acc.: 96.88%] [G loss: 15.614405]\n",
      "815 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.110722]\n",
      "816 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "817 [D loss: 0.096915, acc.: 98.44%] [G loss: 16.118095]\n",
      "818 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614613]\n",
      "819 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.112394]\n",
      "820 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "821 [D loss: 0.000003, acc.: 100.00%] [G loss: 15.755081]\n",
      "822 [D loss: 0.305779, acc.: 95.31%] [G loss: 16.118095]\n",
      "823 [D loss: 0.274763, acc.: 96.88%] [G loss: 16.118095]\n",
      "824 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.477009]\n",
      "825 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "826 [D loss: 0.000003, acc.: 100.00%] [G loss: 15.832184]\n",
      "827 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "828 [D loss: 0.192263, acc.: 98.44%] [G loss: 16.118095]\n",
      "829 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "830 [D loss: 0.027944, acc.: 98.44%] [G loss: 15.620192]\n",
      "831 [D loss: 0.000046, acc.: 100.00%] [G loss: 16.118095]\n",
      "832 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "833 [D loss: 0.165551, acc.: 98.44%] [G loss: 16.118095]\n",
      "834 [D loss: 0.000449, acc.: 100.00%] [G loss: 16.118095]\n",
      "835 [D loss: 1.226724, acc.: 87.50%] [G loss: 16.118095]\n",
      "836 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "837 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "838 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "839 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "840 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "841 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "842 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "843 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614406]\n",
      "844 [D loss: 0.747375, acc.: 95.31%] [G loss: 15.614405]\n",
      "845 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "846 [D loss: 0.013731, acc.: 98.44%] [G loss: 15.110716]\n",
      "847 [D loss: 0.498229, acc.: 96.88%] [G loss: 16.118095]\n",
      "848 [D loss: 0.326147, acc.: 96.88%] [G loss: 16.118095]\n",
      "849 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "850 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "851 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "852 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "853 [D loss: 0.000009, acc.: 100.00%] [G loss: 16.118095]\n",
      "854 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.110714]\n",
      "855 [D loss: 0.000002, acc.: 100.00%] [G loss: 15.615368]\n",
      "856 [D loss: 0.161452, acc.: 98.44%] [G loss: 16.118095]\n",
      "857 [D loss: 0.004218, acc.: 100.00%] [G loss: 16.118095]\n",
      "858 [D loss: 0.127116, acc.: 93.75%] [G loss: 16.118095]\n",
      "859 [D loss: 0.067835, acc.: 98.44%] [G loss: 15.614899]\n",
      "860 [D loss: 0.000013, acc.: 100.00%] [G loss: 15.638865]\n",
      "861 [D loss: 0.171382, acc.: 98.44%] [G loss: 16.003536]\n",
      "862 [D loss: 0.827810, acc.: 92.19%] [G loss: 16.118095]\n",
      "863 [D loss: 0.000003, acc.: 100.00%] [G loss: 15.614617]\n",
      "864 [D loss: 0.001315, acc.: 100.00%] [G loss: 16.118095]\n",
      "865 [D loss: 0.001674, acc.: 100.00%] [G loss: 16.088932]\n",
      "866 [D loss: 0.000309, acc.: 100.00%] [G loss: 15.647639]\n",
      "867 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110714]\n",
      "868 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "869 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110714]\n",
      "870 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "871 [D loss: 0.342914, acc.: 96.88%] [G loss: 16.118095]\n",
      "872 [D loss: 0.000056, acc.: 100.00%] [G loss: 15.935934]\n",
      "873 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "874 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "875 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "876 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.680352]\n",
      "877 [D loss: 0.000024, acc.: 100.00%] [G loss: 15.614405]\n",
      "878 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.103792]\n",
      "879 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.033165]\n",
      "880 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "881 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "882 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "883 [D loss: 0.000021, acc.: 100.00%] [G loss: 16.118095]\n",
      "884 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.614405]\n",
      "885 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.687700]\n",
      "886 [D loss: 0.249915, acc.: 98.44%] [G loss: 16.118095]\n",
      "887 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "888 [D loss: 0.002125, acc.: 100.00%] [G loss: 14.607023]\n",
      "889 [D loss: 0.238269, acc.: 98.44%] [G loss: 15.614405]\n",
      "890 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "891 [D loss: 0.037049, acc.: 98.44%] [G loss: 16.118095]\n",
      "892 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "893 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.117565]\n",
      "894 [D loss: 0.000337, acc.: 100.00%] [G loss: 16.118095]\n",
      "895 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110715]\n",
      "896 [D loss: 0.249111, acc.: 98.44%] [G loss: 16.118095]\n",
      "897 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614576]\n",
      "898 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.607151]\n",
      "899 [D loss: 0.000033, acc.: 100.00%] [G loss: 16.118095]\n",
      "900 [D loss: 0.249127, acc.: 98.44%] [G loss: 15.912588]\n",
      "901 [D loss: 0.027184, acc.: 98.44%] [G loss: 15.668318]\n",
      "902 [D loss: 0.002893, acc.: 100.00%] [G loss: 16.118095]\n",
      "903 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "904 [D loss: 0.174691, acc.: 98.44%] [G loss: 16.118095]\n",
      "905 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.607023]\n",
      "906 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "907 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "908 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.531010]\n",
      "909 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "910 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "911 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "912 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "913 [D loss: 0.066071, acc.: 98.44%] [G loss: 16.118095]\n",
      "914 [D loss: 0.000022, acc.: 100.00%] [G loss: 15.911732]\n",
      "915 [D loss: 1.093634, acc.: 81.25%] [G loss: 16.118095]\n",
      "916 [D loss: 0.498703, acc.: 96.88%] [G loss: 15.614405]\n",
      "917 [D loss: 0.747300, acc.: 95.31%] [G loss: 15.614405]\n",
      "918 [D loss: 0.748486, acc.: 95.31%] [G loss: 15.614405]\n",
      "919 [D loss: 0.000046, acc.: 100.00%] [G loss: 14.696332]\n",
      "920 [D loss: 0.000014, acc.: 100.00%] [G loss: 15.666019]\n",
      "921 [D loss: 0.498206, acc.: 96.88%] [G loss: 15.102230]\n",
      "922 [D loss: 0.747299, acc.: 95.31%] [G loss: 15.110714]\n",
      "923 [D loss: 0.939979, acc.: 93.75%] [G loss: 15.614405]\n",
      "924 [D loss: 1.678804, acc.: 76.56%] [G loss: 12.088572]\n",
      "925 [D loss: 0.747299, acc.: 95.31%] [G loss: 12.592262]\n",
      "926 [D loss: 0.996399, acc.: 93.75%] [G loss: 12.592262]\n",
      "927 [D loss: 0.996399, acc.: 93.75%] [G loss: 13.095953]\n",
      "928 [D loss: 1.744473, acc.: 89.06%] [G loss: 13.599644]\n",
      "929 [D loss: 1.494599, acc.: 90.62%] [G loss: 13.599644]\n",
      "930 [D loss: 1.743698, acc.: 89.06%] [G loss: 14.607024]\n",
      "931 [D loss: 1.743698, acc.: 89.06%] [G loss: 15.113481]\n",
      "932 [D loss: 1.245499, acc.: 92.19%] [G loss: 13.095953]\n",
      "933 [D loss: 1.744381, acc.: 89.06%] [G loss: 13.584500]\n",
      "934 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.095953]\n",
      "935 [D loss: 1.494599, acc.: 90.62%] [G loss: 14.103333]\n",
      "936 [D loss: 1.743698, acc.: 89.06%] [G loss: 11.584883]\n",
      "937 [D loss: 1.494599, acc.: 90.62%] [G loss: 14.515022]\n",
      "938 [D loss: 2.490998, acc.: 84.38%] [G loss: 11.584881]\n",
      "939 [D loss: 2.740099, acc.: 82.81%] [G loss: 14.607023]\n",
      "940 [D loss: 1.245499, acc.: 92.19%] [G loss: 13.095952]\n",
      "941 [D loss: 2.490998, acc.: 84.38%] [G loss: 13.599643]\n",
      "942 [D loss: 2.241898, acc.: 85.94%] [G loss: 12.855947]\n",
      "943 [D loss: 1.743698, acc.: 89.06%] [G loss: 14.103333]\n",
      "944 [D loss: 2.490998, acc.: 84.38%] [G loss: 10.577499]\n",
      "945 [D loss: 1.992802, acc.: 87.50%] [G loss: 12.592262]\n",
      "946 [D loss: 2.490998, acc.: 84.38%] [G loss: 11.584881]\n",
      "947 [D loss: 1.247082, acc.: 92.19%] [G loss: 11.584881]\n",
      "948 [D loss: 1.624822, acc.: 89.06%] [G loss: 15.614405]\n",
      "949 [D loss: 0.996399, acc.: 93.75%] [G loss: 14.607023]\n",
      "950 [D loss: 0.996399, acc.: 93.75%] [G loss: 14.607024]\n",
      "951 [D loss: 1.494599, acc.: 90.62%] [G loss: 13.599644]\n",
      "952 [D loss: 0.249100, acc.: 98.44%] [G loss: 14.607023]\n",
      "953 [D loss: 0.747299, acc.: 95.31%] [G loss: 14.103333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954 [D loss: 0.249100, acc.: 98.44%] [G loss: 14.103333]\n",
      "955 [D loss: 1.743698, acc.: 89.06%] [G loss: 13.599644]\n",
      "956 [D loss: 1.028568, acc.: 92.19%] [G loss: 14.103333]\n",
      "957 [D loss: 0.747299, acc.: 95.31%] [G loss: 15.110714]\n",
      "958 [D loss: 0.996399, acc.: 93.75%] [G loss: 14.103333]\n",
      "959 [D loss: 1.021552, acc.: 92.19%] [G loss: 16.118095]\n",
      "960 [D loss: 0.323703, acc.: 96.88%] [G loss: 15.614406]\n",
      "961 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "962 [D loss: 0.127805, acc.: 93.75%] [G loss: 15.110714]\n",
      "963 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110714]\n",
      "964 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "965 [D loss: 0.481034, acc.: 96.88%] [G loss: 16.118095]\n",
      "966 [D loss: 0.249102, acc.: 98.44%] [G loss: 16.118095]\n",
      "967 [D loss: 1.355927, acc.: 89.06%] [G loss: 15.113728]\n",
      "968 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "969 [D loss: 0.159365, acc.: 98.44%] [G loss: 16.118095]\n",
      "970 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.694711]\n",
      "971 [D loss: 0.251845, acc.: 98.44%] [G loss: 16.118095]\n",
      "972 [D loss: 1.342979, acc.: 90.62%] [G loss: 15.614405]\n",
      "973 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "974 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110714]\n",
      "975 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "976 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "977 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110714]\n",
      "978 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "979 [D loss: 0.249101, acc.: 98.44%] [G loss: 16.118095]\n",
      "980 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "981 [D loss: 0.498200, acc.: 96.88%] [G loss: 14.607023]\n",
      "982 [D loss: 0.747299, acc.: 95.31%] [G loss: 14.607023]\n",
      "983 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "984 [D loss: 0.747299, acc.: 95.31%] [G loss: 16.118095]\n",
      "985 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.508567]\n",
      "986 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "987 [D loss: 0.747299, acc.: 95.31%] [G loss: 16.118095]\n",
      "988 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "989 [D loss: 0.619554, acc.: 95.31%] [G loss: 14.841427]\n",
      "990 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "991 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "992 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "993 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "994 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "995 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "996 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "997 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "998 [D loss: 0.312965, acc.: 96.88%] [G loss: 16.118095]\n",
      "999 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1000 [D loss: 0.251845, acc.: 98.44%] [G loss: 16.118095]\n",
      "1001 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1002 [D loss: 0.251847, acc.: 98.44%] [G loss: 16.118095]\n",
      "1003 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614437]\n",
      "1004 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "1005 [D loss: 0.755536, acc.: 95.31%] [G loss: 16.118095]\n",
      "1006 [D loss: 0.251846, acc.: 98.44%] [G loss: 16.118095]\n",
      "1007 [D loss: 0.640834, acc.: 93.75%] [G loss: 16.118095]\n",
      "1008 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.614405]\n",
      "1009 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.614405]\n",
      "1010 [D loss: 0.249100, acc.: 98.44%] [G loss: 14.607057]\n",
      "1011 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1012 [D loss: 0.498685, acc.: 96.88%] [G loss: 15.614405]\n",
      "1013 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110714]\n",
      "1014 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1015 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.110714]\n",
      "1016 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110714]\n",
      "1017 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110714]\n",
      "1018 [D loss: 0.000001, acc.: 100.00%] [G loss: 14.103333]\n",
      "1019 [D loss: 0.747299, acc.: 95.31%] [G loss: 15.110714]\n",
      "1020 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "1021 [D loss: 0.747299, acc.: 95.31%] [G loss: 15.614405]\n",
      "1022 [D loss: 0.000072, acc.: 100.00%] [G loss: 15.614405]\n",
      "1023 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.614405]\n",
      "1024 [D loss: 0.747299, acc.: 95.31%] [G loss: 15.110714]\n",
      "1025 [D loss: 0.251003, acc.: 98.44%] [G loss: 15.614405]\n",
      "1026 [D loss: 0.520210, acc.: 95.31%] [G loss: 15.614405]\n",
      "1027 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "1028 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "1029 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1030 [D loss: 0.747299, acc.: 95.31%] [G loss: 16.118095]\n",
      "1031 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1032 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "1033 [D loss: 0.000001, acc.: 100.00%] [G loss: 16.118095]\n",
      "1034 [D loss: 0.747299, acc.: 95.31%] [G loss: 14.103333]\n",
      "1035 [D loss: 0.498369, acc.: 96.88%] [G loss: 16.118095]\n",
      "1036 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "1037 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.614405]\n",
      "1038 [D loss: 0.032837, acc.: 98.44%] [G loss: 16.118095]\n",
      "1039 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1040 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1041 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.126806]\n",
      "1042 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1043 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1044 [D loss: 0.249142, acc.: 98.44%] [G loss: 16.118095]\n",
      "1045 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "1046 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1047 [D loss: 0.000412, acc.: 100.00%] [G loss: 16.118095]\n",
      "1048 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1049 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "1050 [D loss: 0.000003, acc.: 100.00%] [G loss: 16.118095]\n",
      "1051 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1052 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "1053 [D loss: 0.293067, acc.: 96.88%] [G loss: 15.614405]\n",
      "1054 [D loss: 0.758299, acc.: 95.31%] [G loss: 16.118095]\n",
      "1055 [D loss: 1.387239, acc.: 89.06%] [G loss: 15.614405]\n",
      "1056 [D loss: 0.223952, acc.: 98.44%] [G loss: 16.118095]\n",
      "1057 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1058 [D loss: 0.019592, acc.: 98.44%] [G loss: 16.118095]\n",
      "1059 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1060 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1061 [D loss: 0.333069, acc.: 96.88%] [G loss: 16.118095]\n",
      "1062 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1063 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1064 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "1065 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.902071]\n",
      "1066 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1067 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1068 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1069 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.620930]\n",
      "1070 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1071 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1072 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1073 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1074 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "1075 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1076 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1077 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.987473]\n",
      "1078 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1079 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614410]\n",
      "1080 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1081 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1082 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1083 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614406]\n",
      "1084 [D loss: 0.487369, acc.: 96.88%] [G loss: 16.118095]\n",
      "1085 [D loss: 0.452066, acc.: 95.31%] [G loss: 16.118095]\n",
      "1086 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1087 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1088 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "1089 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.741791]\n",
      "1090 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1091 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "1092 [D loss: 0.373544, acc.: 96.88%] [G loss: 16.118095]\n",
      "1093 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1094 [D loss: 0.137233, acc.: 98.44%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095 [D loss: 0.009343, acc.: 100.00%] [G loss: 16.118095]\n",
      "1096 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1097 [D loss: 0.009943, acc.: 100.00%] [G loss: 16.118095]\n",
      "1098 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1099 [D loss: 0.249982, acc.: 98.44%] [G loss: 16.118095]\n",
      "1100 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1101 [D loss: 0.330366, acc.: 96.88%] [G loss: 15.614405]\n",
      "1102 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1103 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.762725]\n",
      "1104 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110715]\n",
      "1105 [D loss: 0.435304, acc.: 96.88%] [G loss: 15.273331]\n",
      "1106 [D loss: 0.000003, acc.: 100.00%] [G loss: 15.614405]\n",
      "1107 [D loss: 0.736581, acc.: 93.75%] [G loss: 15.614405]\n",
      "1108 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1109 [D loss: 0.747299, acc.: 95.31%] [G loss: 16.118095]\n",
      "1110 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "1111 [D loss: 1.195842, acc.: 92.19%] [G loss: 15.110714]\n",
      "1112 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.607023]\n",
      "1113 [D loss: 0.868766, acc.: 92.19%] [G loss: 14.607023]\n",
      "1114 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "1115 [D loss: 1.245499, acc.: 92.19%] [G loss: 15.954554]\n",
      "1116 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.110714]\n",
      "1117 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.614405]\n",
      "1118 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.566998]\n",
      "1119 [D loss: 0.747299, acc.: 95.31%] [G loss: 14.607023]\n",
      "1120 [D loss: 0.249122, acc.: 98.44%] [G loss: 15.614405]\n",
      "1121 [D loss: 0.747299, acc.: 95.31%] [G loss: 15.614405]\n",
      "1122 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.133577]\n",
      "1123 [D loss: 0.315517, acc.: 96.88%] [G loss: 15.110714]\n",
      "1124 [D loss: 0.498200, acc.: 96.88%] [G loss: 14.103333]\n",
      "1125 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1126 [D loss: 0.537989, acc.: 95.31%] [G loss: 14.607024]\n",
      "1127 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "1128 [D loss: 2.534011, acc.: 82.81%] [G loss: 15.110933]\n",
      "1129 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.110714]\n",
      "1130 [D loss: 0.340449, acc.: 96.88%] [G loss: 15.614405]\n",
      "1131 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1132 [D loss: 0.249100, acc.: 98.44%] [G loss: 14.606293]\n",
      "1133 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1134 [D loss: 0.249159, acc.: 98.44%] [G loss: 15.110714]\n",
      "1135 [D loss: 0.498303, acc.: 96.88%] [G loss: 16.118095]\n",
      "1136 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110714]\n",
      "1137 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1138 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "1139 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110714]\n",
      "1140 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.607023]\n",
      "1141 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614412]\n",
      "1142 [D loss: 0.498754, acc.: 96.88%] [G loss: 15.614405]\n",
      "1143 [D loss: 0.249129, acc.: 98.44%] [G loss: 15.110714]\n",
      "1144 [D loss: 0.249131, acc.: 98.44%] [G loss: 15.110714]\n",
      "1145 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "1146 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110714]\n",
      "1147 [D loss: 0.000026, acc.: 100.00%] [G loss: 15.614405]\n",
      "1148 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.468437]\n",
      "1149 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1150 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1151 [D loss: 0.000021, acc.: 100.00%] [G loss: 16.118095]\n",
      "1152 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1153 [D loss: 0.747299, acc.: 95.31%] [G loss: 15.614405]\n",
      "1154 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "1155 [D loss: 0.996399, acc.: 93.75%] [G loss: 15.614405]\n",
      "1156 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "1157 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.110714]\n",
      "1158 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110714]\n",
      "1159 [D loss: 0.747299, acc.: 95.31%] [G loss: 16.118095]\n",
      "1160 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1161 [D loss: 0.000018, acc.: 100.00%] [G loss: 15.443360]\n",
      "1162 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1163 [D loss: 0.894039, acc.: 93.75%] [G loss: 16.013916]\n",
      "1164 [D loss: 2.057092, acc.: 85.94%] [G loss: 16.118095]\n",
      "1165 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110714]\n",
      "1166 [D loss: 1.245499, acc.: 92.19%] [G loss: 16.118095]\n",
      "1167 [D loss: 0.747299, acc.: 95.31%] [G loss: 15.110714]\n",
      "1168 [D loss: 0.498200, acc.: 96.88%] [G loss: 14.607023]\n",
      "1169 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.614405]\n",
      "1170 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1171 [D loss: 0.249146, acc.: 98.44%] [G loss: 15.614405]\n",
      "1172 [D loss: 0.747299, acc.: 95.31%] [G loss: 14.104948]\n",
      "1173 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "1174 [D loss: 1.245499, acc.: 92.19%] [G loss: 15.614620]\n",
      "1175 [D loss: 0.040583, acc.: 98.44%] [G loss: 16.118095]\n",
      "1176 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.008388]\n",
      "1177 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.614405]\n",
      "1178 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "1179 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.825274]\n",
      "1180 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.614405]\n",
      "1181 [D loss: 0.747299, acc.: 95.31%] [G loss: 15.614405]\n",
      "1182 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "1183 [D loss: 0.249100, acc.: 98.44%] [G loss: 14.626861]\n",
      "1184 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.110714]\n",
      "1185 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "1186 [D loss: 0.487369, acc.: 96.88%] [G loss: 16.118095]\n",
      "1187 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "1188 [D loss: 0.782084, acc.: 93.75%] [G loss: 15.110714]\n",
      "1189 [D loss: 0.019192, acc.: 98.44%] [G loss: 16.118095]\n",
      "1190 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110714]\n",
      "1191 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "1192 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n",
      "1193 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.148134]\n",
      "1194 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.654154]\n",
      "1195 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110714]\n",
      "1196 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "1197 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.614405]\n",
      "1198 [D loss: 0.249100, acc.: 98.44%] [G loss: 15.110723]\n",
      "1199 [D loss: 0.249100, acc.: 98.44%] [G loss: 16.118095]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=1200, batch_size=32, sample_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Eric)",
   "language": "python",
   "name": "eric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
